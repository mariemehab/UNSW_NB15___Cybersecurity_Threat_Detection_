# -*- coding: utf-8 -*-
"""finaleva_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cQGmfuxFMAxLnXsKhDrFjkr_nfPJyE7C
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
import os

os.chdir(r"D:\AI\NTI summer training\Final Evaluation Data")

# Set the working directory (consider using relative paths or config for portability)
#os.chdir(r"D:\AI\NTI summer training\Final Evaluation Data")

# Load datasets
train = pd.read_csv('UNSW_NB15_training-set.csv')
test = pd.read_csv('UNSW_NB15_testing-set.csv')

train.head()

test.head()

train.info()

test.info()

plt.figure(figsize=(10, 6))
sns.countplot(x='attack_cat', data=train)
plt.title("Distribution of Attack Categories")
plt.xlabel("Attack Category")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

train.describe()
test.describe()

train.isnull().sum()
test.isnull().sum()

# Check for duplicate rows in the training and testing datasets
print("Duplicate rows in Training Data:", train.duplicated().sum())
print("Duplicate rows in Testing Data:", test.duplicated().sum())

train[['label', 'attack_cat']].value_counts()

features = ['dur', 'sbytes', 'dbytes', 'rate']
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
for i, feature in enumerate(features):
    ax = axes[i // 2, i % 2]
    sns.histplot(data=train, x=feature, hue='label', kde=True, ax=ax, element='step')
    ax.set_title(f"Distribution of {feature} by Label")
    ax.set_xlabel(feature)
    ax.set_ylabel("Density")

plt.tight_layout()
plt.show()

sns.countplot(x='label', data=train)
plt.title("üìä Distribution of Normal (0) vs Attack (1)")
plt.show()

corr = train.corr(numeric_only=True)
plt.figure(figsize=(24,24))
sns.heatmap(corr, cmap="coolwarm", annot=True, fmt=".2f")
plt.title("üîó Correlation Matrix with Values")
plt.tight_layout()
plt.show()

# Drop unnecessary 'id' column
train = train.drop('id', axis=1)
test = test.drop('id', axis=1)

# One-hot encode categorical variables
train = pd.get_dummies(train, columns=['proto', 'service', 'state'], drop_first=True)
test = pd.get_dummies(test, columns=['proto', 'service', 'state'], drop_first=True)

# Align train and test sets to ensure same columns
train, test = train.align(test, join='inner', axis=1)

# Separate features and target
X_train = train.drop(['label', 'attack_cat'], axis=1)
y_train = train['label']
X_test = test.drop(['label', 'attack_cat'], axis=1)
y_test = test['label']

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Apply PCA to retain 95% variance
pca = PCA(n_components=0.95)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
print(f"Number of PCA components retained: {pca.n_components_}")
# Check class distribution
print("Class distribution in training set:")
print(y_train.value_counts(normalize=True))

pca_df = pd.DataFrame(X_train_pca[:, :2], columns=['PC1', 'PC2'])
pca_df['label'] = y_train.values

plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue='label', data=pca_df, palette='deep', alpha=0.6)
plt.title("First Two Principal Components Colored by Label")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Label")
plt.show()

# Optional: Handle class imbalance with SMOTE (uncomment if needed)
smote = SMOTE(random_state=42)
X_train_pca, y_train = smote.fit_resample(X_train_pca, y_train)
print("Class distribution after SMOTE:")
print(pd.Series(y_train).value_counts(normalize=True))

#Train KNN with the best k
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_pca, y_train)
y_pred = knn.predict(X_test_pca)

# Evaluate the model
print("‚úÖ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\n‚úÖ Classification Report:")
print(classification_report(y_test, y_pred))
print("\n‚úÖ Accuracy Score:")
print(accuracy_score(y_test, y_pred))

# Visualize confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Oranges')
plt.title("KNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Sample 20% of the training data for hyperparameter tuning
from sklearn.model_selection import train_test_split

# Combine features and labels for sampling
train_data = pd.DataFrame(X_train_pca)
train_data['label'] = y_train.reset_index(drop=True) # Reset index to align

# Sample the combined data
train_sample = train_data.sample(frac=0.2, random_state=42)

# Separate features and labels from the sample
X_train_sample = train_sample.drop('label', axis=1)
y_train_sample = train_sample['label']


#Hyperparameter tuning on the sample
param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=1)  # Use n_jobs=1
grid_search.fit(X_train_sample, y_train_sample)
best_k = grid_search.best_params_['n_neighbors']
print(f"Best k value from GridSearchCV: {best_k}")

#Train KNN with the best k on full data
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train_pca, y_train)
y_pred = knn.predict(X_test_pca)

# Evaluate the model
print("‚úÖ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\n‚úÖ Classification Report:")
print(classification_report(y_test, y_pred))
print("\n‚úÖ Accuracy Score:")
print(accuracy_score(y_test, y_pred))

# Visualize confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Oranges')
plt.title("KNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

model = LogisticRegression(max_iter=1000)
model.fit(X_train_pca, y_train)

y_pred = model.predict(X_test_pca)

print("‚úÖ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\n‚úÖ Classification Report:")
print(classification_report(y_test, y_pred))

print("\n‚úÖ Accuracy Score:")
print(accuracy_score(y_test, y_pred))

plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("üîç Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier(splitter='best', max_depth=3, random_state=42)
dt_model.fit(X_train_pca,y_train)
predictdt_y = dt_model.predict(X_test_pca)
accuracy_dt = dt_model.score(X_test_pca,y_test)
print("Decision Tree accuracy is :", accuracy_dt)

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=4)
rf_model.fit(X_train_pca,y_train)
predictrf_y = rf_model.predict(X_test_pca)
accuracy_rf = rf_model.score(X_test_pca,y_test)
print("Random Forest Classifier accuracy is :", accuracy_dt)

from sklearn.ensemble import AdaBoostClassifier
model=AdaBoostClassifier(n_estimators=3,random_state=42)
model.fit(X_train_pca,y_train)
y_pred=model.predict(X_test_pca)
print("Accuracy:", model.score(X_test_pca, y_test))

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_train_pca, y_train, cv=5)
print("Cross-validation scores:", scores)
print("Mean cross-validation score:", np.mean(scores))

from xgboost import XGBClassifier

#Set objective and num_class for multiclass classification
num_classes = len(np.unique(y_train))
model = XGBClassifier(n_estimators=100, random_state=42, objective='multi:softmax', num_class=num_classes)
model.fit(X_train_pca, y_train)
y_pred = model.predict(X_test_pca)
print("Accuracy:", model.score(X_test_pca, y_test))
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model,X_train_pca , y_train, cv=5)
print("Cross-validation scores:", scores)
print("Mean cross-validation score:", np.mean(scores))

models = {
    "KNN": accuracy_score(y_test, knn.predict(X_test_pca)),
    "Logistic Regression": accuracy_score(y_test, model.predict(X_test_pca)),  # from LogisticRegression section
    "Decision Tree": dt_model.score(X_test_pca, y_test),
    "Random Forest": rf_model.score(X_test_pca, y_test),
    "AdaBoost": model.score(X_test_pca, y_test),  # from AdaBoost section
    "XGBoost": model.score(X_test_pca, y_test)   # from XGBoost section
}

# Plotting
plt.figure(figsize=(10, 6))
sns.barplot(x=list(models.keys()), y=list(models.values()))
plt.title("Model Accuracy Comparison")
plt.xlabel("Model")
plt.ylabel("Accuracy Score")
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
svc=SVC()
#declare parameters for hyperparameter tuning
parameters = [ {'C':[1, 10, 100, 1000], 'kernel':['linear']},
               {'C':[1, 10, 100, 1000], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},
               {'C':[1, 10, 100, 1000], 'kernel':['poly'], 'degree': [2,3,4] ,'gamma':[0.01,0.02,0.03,0.04,0.05]}
              ]

grid_search = GridSearchCV(estimator = svc,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 5,
                           verbose=0)


grid_search.fit(X_train_pca, y_train)
print("Best parameters found: ", grid_search.best_params_)

